<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
<meta http-equiv="Cache-Control" content="no-siteapp">
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=1, minimum-scale=1, maximum-scale=1">
<meta name="renderer" content="webkit">
<meta name="google" value="notranslate">
<meta name="robots" content="index,follow">


<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="夜凉如水">
<meta name="twitter:description" content="人丑就要多读书">
<meta name="twitter:image:src" content="http://chenye-thu.github.io/images/avatar.png">

<meta property="og:url" content="http://chenye-thu.github.io">
<meta property="og:title" content="夜凉如水">
<meta property="og:description" content="人丑就要多读书">
<meta property="og:site_name" content="夜凉如水">
<meta property="og:image" content="http://chenye-thu.github.io/images/avatar.png">
<meta property="og:type" content="website">
<meta name="robots" content="noodp">

<meta itemprop="name" content="夜凉如水">
<meta itemprop="description" content="人丑就要多读书">
<meta itemprop="image" content="http://chenye-thu.github.io/images/avatar.png">

<link rel="canonical" href="http://chenye-thu.github.io">

<link rel="shortcut icon" href="/favicon.png">
<link rel="apple-itouch-icon" href="/favicon.png">
<link rel="stylesheet" href="/bundle/index.css">
<script type="text/javascript">
    var timeSinceLang = {
        year: '年前',
        month: '个月前',
        day: '天前',
        hour: '小时前',
        minute: '分钟前',
        second: '秒前'
    };
    var root = '';
</script>


        <meta name="keywords" content="机器学习,数据挖掘,Python,频繁模式挖掘,编程,">
        <meta name="description" content="机器学习课第三次作业记录">
        <meta name="author" content="夜凉如水">
        <title>机器学习课第三次作业记录</title>
    </head>
    <body>
        <article class="container">
            <header class="header-wrap">
  <a class="index" href="/">
    <img class="logo" src="/images/avatar.png" />
    夜凉如水
  </a>
  <ul class="menu">
      <li class="menu-item"><a href="/archive.html">归档</a></li>
      <li class="menu-item"><a href="/tag.html">标签</a></li>
      <li class="menu-item"><a href="/atom.xml">订阅</a></li>
  </ul>
</header>

            <article class="main article">
                <h1 class="title">机器学习课第三次作业记录</h1>
                <section class="info">
                    <span class="avatar" style="background-image: url(/images/icon.png);"></span>
                    <a class="name" href="/about.me.html">夜凉如水</a>
                    
                    <span class="date" data-time="1514700000"><span class="from"></span></span>
                    
                    <span class="tags"><a class="tag" href="/tag/%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0/index.html">机器学习</a><a class="tag" href="/tag/%e6%95%b0%e6%8d%ae%e6%8c%96%e6%8e%98/index.html">数据挖掘</a><a class="tag" href="/tag/Python/index.html">Python</a><a class="tag" href="/tag/%e9%a2%91%e7%b9%81%e6%a8%a1%e5%bc%8f%e6%8c%96%e6%8e%98/index.html">频繁模式挖掘</a><a class="tag" href="/tag/%e7%bc%96%e7%a8%8b/index.html">编程</a></span>
                </section>
                <article class="content"><blockquote>
<p>2017-12-13 周三 晴 北京 清华大学 开始<br>
2017-12-17 周日 晴 北京 清华大学 完成一半<br>
2017-12-29 周五 霾 北京 清华大学 开始另一半<br>
2017-12-31 周日 晴 北京 清华大学 完成</p>
</blockquote>

<h2>0 作业内容</h2>

<p>数据集DBLP 数据库中的IJCAI, AAAI, COLT, CVPR, NIPS, KR, SIGIR, SIGKDD八个会议中从2007年至今的所有数据。也可在DBLP中适当扩展数据集，但这八个会议必须包含在内。</p>

<ul>
<li><p><strong>任务1</strong><br>
（1-1）每⼀个会议都有各⾃的⽀持者，现在请你将每个会议各⾃的研究者寻找出来，并且根据时间信息，看看哪些⼈依然活跃，哪些⼈不再活跃。<br>
（1-2）在找到各⾃的研究者群体后，我们希望找到经常性在⼀起合作的学者，将之称为‘团队’。请你根据研究者合作发表论⽂次数为根据进⾏频繁模式挖掘，找出三个⼈以上的‘团队’。</p></li>

<li><p><strong>任务2</strong><br>
（2-1）每⼀篇论⽂都会涉及到⼀个或多个主题，请你先定出主题词，然后根据每个‘团队’发表的论⽂的情况，提炼出这个团队最常涉猎的主题。<br>
（2-2）团队和主题多是会随着时间⽽动态变化。请你根据⾃⼰所定的时间段（五年，三年，两年或是⼀年）描述团队的构成状况以及其研究主题的变化情况。</p></li>
</ul>

<p><strong>题目分析</strong>：</p>

<ol>
<li>任务（1-1）貌似不需要什么频繁模式挖掘，对于每个会议的数据，提取<strong>作者</strong>与<strong>时间</strong>信息，<strong>同一作者出现次数多的</strong>即为会议的支持者。再根据不同时间段的出现次数，可以知道这些人是否依然活跃。</li>
<li>任务（1-1）目前的<strong>难点</strong>在于：一是<strong>数据量大，</strong>按照每篇论文2位作者，即每条信息6行计算，大约有199992/6=3.3万条论文信息。二是<strong>数据为文本信息</strong>，提取的工作怎么进行？直接处理字符串？对名字编号？还是HashMap？别忘了后面还会寻找主题词呢，这可就真的是文本了。所以，有没有什么<strong>工具</strong>？12.14日：查了一下，共有27454篇文章。</li>
<li>任务（1-2）是在（1-1）的基础上，对于寻找到的这些研究者后，寻找多次一起发文章的团队。这就需要频繁模式挖掘了，其实就是找到在一起发文章次数多的这些人。那么是直接找三人以上法文章的，还是说两个人同时与另外一个人都发文章多也算？这个问题是次要的。</li>
<li>任务（2-1）让我们先定出<strong>主题词</strong>，这个主题词是从<strong>文章标题</strong>中获得的，只需要文章标题。那么一个单词算是主题词吗？有同学说主题词都是些learning、model之类的，是不是不够好？我还没有自己找一下试试呢。另外，有同学说，貌似有<strong>主题抽取算法</strong>，不知道能不能找到。</li>
<li>然后，任务（2-1）要求根据每个团队发表论文的情况，提炼出其最常涉猎的主题。题目也说了，<strong>每一篇论文都会涉及一个或多个主题</strong>，那么问题更清楚了，即：<strong>对于每一篇文章，都定出其主题词；然后对于某团队的文章，找到出现次数多的主题词</strong>。</li>
<li>任务（2-2）与（1-1）第二个要求类似，即再对不同时间段内的文章，分别研究团队的研究主题。</li>
<li>任务（2-2）还说了，还要<strong>研究团队的构成状况</strong>，这个貌似就比较难：对于一个团队，如果走了一个人，那么这还是原来的团队吗？如果说团队一直在增加人数，应该算是一个团队，但是到底要怎样处理呢？</li>
<li>以上面内容来看，可以先不管任务2，先做任务1即可。做任务2时，可以先调研主题抽取算法。</li>
<li>那么首要的问题就是找工具还是自己编？先找工具吧！</li>
<li>但是我又突然想到一点，能不能通过所有文章，定出有限的主题词，然后对于每一个文章标题，通过机器学习-预测的方式，得到它的主题词呢？可能行不通？优点想远了。</li>
<li>所以说还是要找工具，或者说，先解决数据的问题，因为都是字符串，看看有没有能搞定的工具。答：查到的工具可以处理字符串，没问题。</li>
<li><strong>12.14日</strong>：查到的<strong>pyfpgrowth</strong>工具很给力啊，找频繁项集没有问题！目前数据量来说不怎么影响，运算很快！</li>
<li>现在任务1基本上已经解决了，后面就是要根据时间信息来处理了。这里还是存在一个难点，那就是研究者与时间的绑定问题。频繁项集挖掘得到的结果是名字，那么怎样去找到这个名字下的文章呢？我们也来使用<strong>字典</strong>把作者与其文章序号绑定？答：字典搞定，行得通！</li>
<li>任务1算是完成基本要求了，找到了频繁的作者，也匹配了他们的文章，也就找到了活跃时间；然后又找到了一起发文章多的团体。不过<strong>没有匹配团队的文章</strong>：如果只找共同发过的文章，还是需要写一些程序的；如果找每个人发过的文章，那么就容易很多。</li>
<li>后面任务2就是要找主题词了。对于团队的主题词来说，可能先<strong>把所有的论文（即不一定是共同发表的）都放进来，然后定出其主题词</strong>。</li>
<li>任务（2-2）先抛开团队构成情况不管。那么解决了任务（2-1），再分时间段分析，就完成了。所以说，现在<strong>重点就是任务（2-1）</strong>的要求。</li>
<li>对于主题词的获取我想到了一个思路（单个单词的）：<strong>把每篇文章的标题看做一个事务，每个单词看为一项，进行频繁项集挖掘</strong>。当然，标题内的副词之类的（the、on、to……）要去掉。试试吧。还有，标点符号（:）怎样去掉？答：去掉已经去掉了。上面的思路也实现得差不多。能够得到一些关键词。</li>
<li>下面就是<strong>拿到某个团队所有文章的标题</strong>，再确定其关注的主题。这个怎样与已经获得的主题进行匹配？任务（2-1）的要求还是得仔细推敲一下。</li>
<li><strong>12.15日</strong>：今天又做了一些处理，把原数据中一些没有作者的奇怪文章去掉。另外又加了一些介词，试图把标题清理得更干净些。</li>
<li>还是第18条中讲的，<strong>怎样将这些关键词与团队关键词进行匹配</strong>？</li>
<li>还有，我<strong>想找到最频繁的数据，在字典里怎样去找</strong>？</li>
<li>然后就是要<strong>找到一个团队的所有文章</strong>，来抽取关键词。答：这个已经实现得差不多了。</li>
<li>从现在来看，因为一个团队发的文章怎么也有几篇，<strong>找到出现2次以上的关键词基本上是没有问题</strong>的。那么由此看来，当需要找某个团队全部论文的关键词时，不一定需要进行匹配。而在分了具体年月后，可能文章数目比较少，不容易获得关键词，就需要匹配了。</li>
<li>下面就试试能不能分时间段来分析。07到17共11年，先按照两年分析？</li>
<li>刚才看群里，助教说，关键词可以用<strong>nlp方法</strong>来获得。但是这是啥玩意啊。我查到sklearn貌似可以做nlp，什么<strong>词向量特征构建</strong>之类的：<a href="http://blog.csdn.net/stevenkwong/article/details/52576266">利用sklearn做自然语言处理（NLP）——词向量特征构建</a>。那就看看例子是啥东西。</li>
<li>我又想到一个点子，那就是，从全部论文获得的关键词，特别是单个词语，根据其出现的<strong>次数/比例</strong>，为这个词语添加一项<strong>权重</strong>，这一个权重呢，会对我们最终的结果产生影响。比如，就让大权重的词语的支持度再大一些。</li>
<li>接上条，我可以试试看看是不是每篇文章标题都有频繁的词汇。答：对于IJCAI会议的3859篇文章标题，统计出现10及以上的单词，定义为频繁单词或者关键词。再对每个文章标题，统计其中出现关键词的次数，得到的结果中，只有123篇没有频繁单词。这说明我的方法还是行得通的。</li>
<li>现在我想到的方法是：<strong>对于单篇文章</strong>标题来说，如果要确定其关键词，那么就选其中<strong>前两到三个最频繁的单词。</strong>而<strong>对于团队文章</strong>呢，就不一样了，一是要参考<strong>单词是否频繁</strong>，还要参考<strong>单词在团队论文中出现的频次</strong>，其实这两项呢，可以<strong>互为权重</strong>。另外，对于团队来说，还要参考其频繁的多单词模式。<strong>12.16日</strong>：目前这个<strong>权重是使用两者相乘</strong>的方法。我感觉还是有一定意义的。我使用<code>sorted</code>函数，对频繁单词进行了排序，可以得到最频繁的单词，即团队的主题。</li>
<li>刚刚又想到一点，有的学者是多个会议的支持者，对于他们的文章要总体研究吗？不过这个问题现在看还是比较简单的，相比于单个会议的情况，只不过是把数据集扩大而已。这个后面如果有时间，可以总体来看的。</li>
<li>对于<strong>没有频繁单词的文章</strong>呢？怎么办？选最长的一个？哈哈</li>
<li><strong>不同会议的文章数目不同</strong>，在选择频繁阈值的时候，是不是要对此进行考虑？</li>
<li><strong>12.16日</strong>：下面就是对团队的论文进行分时间段的分析。要注意两点：一是如果时间段内文章很少，比如就一篇，那么其主题怎么选？应该选出现频率最高的那个词。但是如果恰好这篇文章又没有频繁单词，怎么办？<strong>鉴于这种情况出现的概率非常小，我们可以认为，此时间段内，没有研究主题</strong>。</li>
<li>07到17共11年，先按照两年分析？因为11是一个质数，怎么分呢？先按照三年来分吧：07~09，10~12，13~15，16~17。我估计16、17的文章可能多一些，所以先这样吧，四个时间段。</li>
<li>分时间段后获取关键词也完成了，反正就是某个时间段有关键词最好，没有那就没有，无所谓。</li>
<li>现在对于某个时间段只有一篇文章的情况还没处理。可以把这个加上。加是加上了，现在感觉这个倒不是大问题。我反而觉得这个还不如不加呢。三年就这么一篇文章，还谈什么主题。是吧，不行这个先不要了。</li>
<li><strong>12.17日</strong>：现在基本的工作已经做得差不多了。现在只是做了一个团队，后面可能需要完善程序，一下子把所有团队数据搞出来。因为12.25日考试，这个作业现在就先暂停。等到29号考完试之后，再进行下一步工作。</li>
<li>当然还有一点，那就是关于任务（2-2）中要求的不同时间段的<strong>“团队的构成状况”</strong>。对于这一点，本来我觉得是比较难处理的。我想到的一个方案就是，<strong>把团队每个时间段内文章的所有作者拿出来，以这个作为团队的构成</strong>。也就是说，最开始寻找的三个人以上的团队，这里面的三个人可以认为是团队的“核心”成员，而其他的人呢，也算是团队的构成，从这样来看呢，也算是一种解释对吧。反正，有比没有好。在这突然就想到建模竞赛的时候，一般最后一题很难做，但是总得做点什么呀，编也得编出来，自己能够说通，让人听上去有一定道理就行。</li>
<li>对了，我还想起一个问题，就是减小阈值的时候，反而论文标题带频繁单词的数目减少了，不知道是什么问题，需要研究下。</li>
<li>作业到此暂停。再回来的时候，需要先完成36、37、38条中的内容。</li>
<li><strong>12.29日</strong>：到今天，所有考试都结束了，现在剩下这一个作业。按照原定计划，应该把36~38条的内容完成。而且，现在就要考虑了，最后的<strong>结果是怎样的一个呈现形式</strong>？得到的结果是很多的。我感觉每个小题的结果应该独立给出。比如<strong>任务（1-1）</strong>给出，<strong>每个会议支持者的名字、文章数目以及每篇文章的时间</strong>；<strong>任务（1-2）</strong>给出，<strong>每个会议支持者团队内名字、合作发表的文章数目</strong>；<strong>任务（2-1）</strong>给出，<strong>每个会议支持者团队内名字、所有文章数目、涉及的主题词（包括单个的和多个词的、加权不加权的）及其次数</strong>；<strong>任务（2-2）</strong>给出，<strong>每个会议支持者团队内名字、不同时间段内的主题词以及核心成员以外的作者</strong>。</li>
<li><strong>12.31日</strong>：按照40条的要求，已经将任务（1-1）和（1-2）的结果保存完毕。</li>
<li>任务（2-1）的结果，包括频繁单词和频繁多词，以及加权重和不加权重的，这样来说，对每个团队就包括四个结果。怎样保存呢？四种结果感觉还是按照字典的方式比较好看些。答：任务（2-1）的结果已经保存完毕。<strong>在KDD中韩佳炜团队的主题词结果中，明显能看出权重与非权重的区别。</strong></li>
<li>团队的作者结果中还有一点小问题，就是其中有些三人的团队还合并成了四人以上的团队，这就存在一定的重复。不过这不是太大的问题，可以先不去管他。考虑到实际得到的团队数量并不多，写程序判断一下，将重复的去掉也是可以的，这个看后面有没有时间吧。</li>
<li>任务（2-2）的结果是在（2-1）的基础上更加复杂了，分了时间段。那么对每一个时间段，分别保存其主题词。答：（2-2）的主题词结果已经处理完了，而且也加上了对只有一篇情况的处理。</li>
<li>其实还漏了一点，那就是如果两三篇文章没有重复单词的情况。答：这个问题现在也已经解决了。<strong>从这可以看出权重的用处了，如果只有一篇文章，或者几篇文章没有重复的单词，那么就没有频繁模式可言，也就无法判断哪个词是主题词。而本文通过对权重的引入，能够对每个单词的“重要性”进行比较，进而从中选择出主题词。</strong>例子：在*SIGIR<em>会议中，团队</em>(&lsquo;Alistair Moffat&rsquo;, &lsquo;Justin Zobel&rsquo;, &lsquo;William Webber&rsquo;)*在2010 ~ 2012年有2篇文章，但是没有重复单词。。**在*IJCAI<em>会议中，团队</em>(&lsquo;Christian Bessiere&rsquo;, &lsquo;George Katsirelos&rsquo;, &lsquo;Toby Walsh&rsquo;)*在2010 ~ 2012年有1篇文章。</li>
<li>好了，接下来就是再把团队信息加上了。答：很简单，信息已经加上了。至此，题目已经基本上完成了。还存在的问题的话，就是对于38条，不过这个我暂时不想整了。我觉得这次作业做到这个程度已经可以了，毕竟只是一次作业，没有必要太较真。后面两天多花点时间，把报告写得好一点，这才是重要的。*<strong>KDD*会议中*韩佳炜*团队是一个很好的案例</strong>。</li>
</ol>

<h2>1 数据集</h2>

<ol>
<li>原始数据中的有用信息也就是作者、标题、时间、会议四项内容，因此，助教筛选的数据信息是足够的。也就是说<strong>主题词只能从标题里面定</strong>了。</li>
<li>题目中说的一个会议是SIGKDD，而助教给的是KDD，两者一样吗？答：<strong>一样</strong>，已经确认。</li>
<li>我看数据集的文档中，有“Person”这类数据，不知道会不会有用。</li>
<li>八个会议分别是什么可能首先要搞清楚。答：见下面的表格。</li>
<li>从题目要求来看，<strong>八个会议之间没有什么关系</strong>，所以各自分开处理即可。</li>
<li>12.14日：因为昨天找到的工具，可以直接对字符串进行频繁项集挖掘，所以现在首要的任务是对助教提供的数据集进行处理，把数据保存成需要的形式。首先第一步，应该<strong>把数据分为八个会议的数据</strong>。</li>
<li>那么是用Python来处理？还是说用C++？用C++建个类的话，可能也比较好处理。Python的话肯定也行啊，但是不太熟。学一下Python的class？</li>
<li>使用Python建立了类，并且从文本中获取文章信息，27454篇文章的信息瞬间读取完毕，速度还是很给力的。</li>
<li>下面就好做了，先提取一个会议的数据看看。</li>
</ol>

<table>
<thead>
<tr>
<th>会议</th>
<th>全称</th>
<th>主要方向</th>
</tr>
</thead>

<tbody>
<tr>
<td>IJCAI</td>
<td>International Joint Conferences on Artificial Intelligence</td>
<td>AI综合</td>
</tr>

<tr>
<td>AAAI</td>
<td>Association for the Advancement of Artificial Intelligence</td>
<td>AI</td>
</tr>

<tr>
<td>COLT</td>
<td>Annual Conference on Learning Theory</td>
<td>学习</td>
</tr>

<tr>
<td>CVPR</td>
<td>IEEE Conference on Computer Vision and Pattern Recognition</td>
<td>计算机视觉</td>
</tr>

<tr>
<td>NIPS</td>
<td>Conference and Workshop on Neural Information Processing Systems</td>
<td>神经信息处理</td>
</tr>

<tr>
<td>KR</td>
<td>Principles of Knowledge Representation and Reasoning</td>
<td>知识表示和推理</td>
</tr>

<tr>
<td>SIGIR</td>
<td>Special Interest Group on Information Retrieval</td>
<td>信息检索</td>
</tr>

<tr>
<td>SIGKDD</td>
<td>Special Interest Group on Knowledge Discovery and Data Mining</td>
<td>数据挖掘</td>
</tr>
</tbody>
</table>

<h2>2 频繁模式挖掘</h2>

<ol>
<li>关联规则挖掘：先寻找<strong>频繁项集</strong>，再找<strong>关联规则</strong>。频繁项集只与支持度（即出现的频率）有关，关联规则与支持度和置信度（条件概率）都有关。</li>
<li>频繁模式与关联规则在这是一回事吗？答：不一样。频繁模式里面包含频繁项集、关联规则等。或者说，频繁模式更多的对应的是频繁项集？</li>
<li>在任务（1-2）中，其实是频繁项集挖掘。对于此问题，每篇文章的多个作者名字作为一个事务。</li>
<li>其实对于任务（1-1）也可以认为是频繁项集挖掘，只不过挖掘的是频繁的“1-项集”而已。对于这个问题，每个作者的名字作为一个事务。</li>
<li>12.17日：对于FP-Growth算法，感觉这篇文章讲的可以看懂：<a href="https://www.cnblogs.com/datahunter/p/3903413.html">关联分析：FP-Growth算法</a>。看过这篇博文后，我对FP-Growth算法基本上算是明白了。如果出一个简单的题目的话，我应该可以算出来。</li>
<li>对于Apriori算法，参考这篇博文：<a href="http://blog.csdn.net/baimafujinji/article/details/53456931">数据挖掘十大算法之Apriori详解</a>。说得还是比较清楚的。这个算法比较容易看懂，看来容易的算法计算量可能就会大。</li>
<li>我看老师的课件里，对于Apriori算法还有一个优化的算法，为了减少扫描数据库次数的，叫DIC。我知道了，DIC的基本思路就是：把数据分成几个部分，类似于公交站点。每一站就更新项集的支持度。但是只要子集的支持度超过了阈值（此时可能其支持度还没算完），那么就可以得到候选的父集，在下一站计算时，父集就可以参与支持度的计算了。这样来看，每一次完整扫描，不仅仅扫描了k-项集，还有可能扫描k+1-项集，k+2-项集，所以减少了扫描的次数。</li>
</ol>

<h2>3 工具</h2>

<ol>
<li>查到了一个FP-Growth算法的Python工具：<a href="https://pypi.python.org/pypi/pyfpgrowth/1.0">pyfpgrowth 1.0</a>，不知道这个能不能处理字符串型数据。答：经测试，可以处理字符串，貌似OK！测试可用啊，还可以啊！</li>
<li>查到了一个FP-Growth算法的Matlab工具： <a href="http://cn.mathworks.com/matlabcentral/fileexchange/52868-fp-growth-association-rule-mining">FP-Growth Association Rule Mining</a></li>
</ol>

<h2>4 Python相关</h2>

<ol>
<li>Python建立类。从这里学的：<a href="http://www.runoob.com/python/python-object.html">Python 面向对象</a>。</li>
<li>Python打开文本文件遇到 <strong><em>UnicodeDecodeError: &lsquo;gbk&rsquo; codec can&rsquo;t decode byte 0x11 in position 3111: illegal multibyte sequence</em></strong> 这样的问题，说明文件打开的编码格式不对。在我的软件里，默认使用的是<code>gbk</code>格式，然而对于所给的数据集，使用的是<code>utf-8</code>格式，所以在文件打开时，使用<code>f = open(&quot;FilteredDBLP.txt&quot;, 'r', encoding='utf-8')</code>，即添加一个<code>encoding</code>参数即可。</li>
<li><a href="http://www.runoob.com/python/python-dictionary.html">Python 字典(Dictionary)</a>：即“key-&gt;value”的组合。搜到的<strong>pyfpgrowth</strong>返回的频繁项集就是字典。</li>
<li><a href="http://www.runoob.com/python/python-tuples.html">Python 元组（Tuple）</a>：Python的元组与列表类似，不同之处在于元组的元素不能修改。元组使用小括号，列表使用方括号。元组创建很简单，只需要在括号中添加元素，并使用逗号隔开即可：<code>tup1 = ('physics', 'chemistry', 1997, 2000)</code>。元组中只包含一个元素时，需要在元素后面添加逗号<code>tup1 = (50,)</code>。搜到的<strong>pyfpgrowth</strong>返回的频繁项集是字典，字典的键值（key）是用元组表示的。</li>
<li>判断字典<code>dict</code>中是否存在键值<code>key</code>，只需使用<code>key in dict</code>即可，返回bool值。</li>
<li>字典的遍历可以使用<code>for key, value in patterns.items()</code>。</li>
<li>Python如何判断一个元组（列表）是否完全包含另一元组（列表），参考：<a href="https://www.cnblogs.com/for-my-life/p/8038250.html">https://www.cnblogs.com/for-my-life/p/8038250.html</a>。将元组或列表转换为<code>set</code>类型，使用<code>issubset()</code>函数：<code>set(list1).issubset(set(list2))</code>。</li>
<li>Python将字符串中的标点符合去掉：<code>re</code>即正则表达式，<code>re.sub()</code>即替换，</li>
</ol>

<pre><code class="language-python">import re
from string import punctuation
a = &quot;a b ccc: d, d&quot;
delSet = punctuation
# delSet = &quot;.:()&quot;
b = re.sub(&quot;[&quot;+delSet+&quot;]&quot;, &quot;&quot;, a)
print(b)
</code></pre>

<ol>
<li>Pyhton的字符串转换为小写：<code>str1 = str.lower()</code>。</li>
<li>Python去掉字符串两端的某符号(如空格)：<code>str1 = str.strip(' ')</code>。</li>
<li>Python的逻辑运算符是<code>and</code>、<code>not</code>、<code>or</code>。</li>
<li><a href="https://www.cnblogs.com/technologylife/p/5628582.html">Python&ndash;关于dict</a>：dict 的查找速度快，无论dict有10个元素还是10万个元素，查找速度都一样。而list的查找速度随着元素增加而逐渐下降。 dict的缺点是占用内存大，还会浪费很多内容，list正好相反，占用内存小，但是查找速度慢。</li>
<li>Lambda表达式：参考<a href="https://www.zhihu.com/question/20125256">Lambda 表达式有何用处？如何使用？</a>。</li>
<li>Python中访问列表最后一个元素，其id为-1。而如果想要访问列表<code>list</code>的最后几个元素（如最后3个），可以使用<code>list[-3:]</code>。即<code>:</code>后面不加数字就认为是到列表最后。</li>
<li><a href="https://www.cnblogs.com/sysu-blackbear/p/3283993.html">Python中sorted函数的用法</a>。</li>
<li><a href="https://zhidao.baidu.com/question/493976892415209292.html">Python 字典输出value最大值所对应的key怎么实现</a>。</li>
<li>Python读写文本文件，参考<a href="http://www.runoob.com/python/python-files-io.html">Python 文件I/O</a>。下面是写文本文件的例子：</li>
</ol>

<pre><code class="language-python">f = open(&quot;output.txt&quot;, 'w')
f.write(&quot;Articles count: %d&quot; % 22)
f.close()
</code></pre>

<ol>
<li>两个list组成字典：<a href="http://blog.csdn.net/caroline_wendy/article/details/47060459">Python - 两个列表(list)组成字典(dict)</a>。使用<code>dictionary = dict(zip(keys, values))</code>，其中<code>keys</code>和<code>values</code>是两个list。</li>
</ol>
</article>
                <section class="author">
                    <div class="avatar" style="background-image: url(/images/icon.png);"></div>
                    <a class="name" href="/about.me.html">夜凉如水</a>
                    <div class="intro">人丑就要多读书</div>
                </section>
                <section class="recommend">
                    
                    <section class="nav prev more">
                        <div class="head">上篇文章</div>
                        <a class="link" href="/article20171229-markdown-equation.html">Markdown中加入公式</a>
                    </section>
                    
                    
                    <section class="nav next more">
                        <div class="head">下篇文章</div>
                        <a class="link" href="/article20171114-scikit-learn-02.html">基于Python的机器学习框架scikit-learn学习笔记02</a>
                    </section>
                    
                </section>
                
    <section id="disqus_thread"></section>
    <script type="text/javascript">
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//username.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>


            </article>
        </article>
        <footer class="footer">
    <span class="copyright">
        夜凉如水 ©
        <script type="text/javascript">
            document.write(new Date().getFullYear());
        </script>
    </span>
    <span class="publish">Powered by <a href="http://www.chole.io/" target="_blank">Ink</a></span>
</footer>

        <script src="/bundle/index.js"></script>
    </body>
</html>
